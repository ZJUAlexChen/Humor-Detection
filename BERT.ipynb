{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI6103 Deep Learning & Applications Project"
      ],
      "metadata": {
        "id": "jAuJLAlAGe-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation"
      ],
      "metadata": {
        "id": "lXypeYpmGp2s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8YrGotDGWpV"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi"
      ],
      "metadata": {
        "id": "25NdvdciIjdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "v1rAXMnIpD4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import zipfile\n",
        "import time\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import transformers\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "MM2ZPAplQp72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# path setting\n",
        "# PROJECT_PATH = \"./drive/MyDrive/DL_Project\"\n",
        "PROJECT_PATH = os.getcwd()\n",
        "DATA_PATH = \"data\"\n",
        "TRAIN_DATA_PATH = \"semeval-2020-task-7-dataset/subtask-1/train.csv\"\n",
        "TEST_DATA_PATH = \"semeval-2020-task-7-dataset/subtask-1/test.csv\"\n",
        "VAL_DATA_PATH = \"semeval-2020-task-7-dataset/subtask-1/dev.csv\"\n",
        "FIG_PATH = \"figures\"\n",
        "MODEL_PATH = \"models\"\n",
        "RES_CSV_PATH = \"res_csvs\"\n",
        "CURR_MODEL_PATH = \"\"\n",
        "\n",
        "# generate folders if not exist\n",
        "if not os.path.exists(os.path.join(PROJECT_PATH, DATA_PATH)):\n",
        "    os.mkdir(os.path.join(PROJECT_PATH, DATA_PATH))\n",
        "if not os.path.exists(os.path.join(PROJECT_PATH, FIG_PATH)):\n",
        "    os.mkdir(os.path.join(PROJECT_PATH, FIG_PATH))\n",
        "if not os.path.exists(os.path.join(PROJECT_PATH, MODEL_PATH)):\n",
        "    os.mkdir(os.path.join(PROJECT_PATH, MODEL_PATH))\n",
        "if not os.path.exists(os.path.join(PROJECT_PATH, RES_CSV_PATH)):\n",
        "    os.mkdir(os.path.join(PROJECT_PATH, RES_CSV_PATH))\n",
        "\n",
        "# model settings\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "MAX_LEN = 256\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "EPOCH_NUM = 20\n",
        "LR = 1e-4\n",
        "BATCH_SIZE = 32\n",
        "HUMOR_THRESHOLD = 0.94"
      ],
      "metadata": {
        "id": "NbZWUwrifsa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Humicroedit"
      ],
      "metadata": {
        "id": "dpNMsnRuJExT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download dataset\n",
        "# url references: https://huggingface.co/datasets/humicroedit\n",
        "\n",
        "data_url = \"https://cs.rochester.edu/u/nhossain/semeval-2020-task-7-dataset.zip\"\n",
        "data_path = os.path.join(PROJECT_PATH, DATA_PATH)\n",
        "\n",
        "response = requests.get(data_url)\n",
        "zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "zip_file.extractall(path=data_path)"
      ],
      "metadata": {
        "id": "Hqj95tkLQNmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn data into dataframes\n",
        "'''\n",
        "train set: 9652 rows\n",
        "test set: 3042 rows\n",
        "validation set: 2419 rows\n",
        "columns: id | original | edit | grades | meanGrade\n",
        "'''\n",
        "\n",
        "train_df = pd.read_csv(os.path.join(PROJECT_PATH, DATA_PATH, TRAIN_DATA_PATH))\n",
        "test_df = pd.read_csv(os.path.join(PROJECT_PATH, DATA_PATH, TEST_DATA_PATH))\n",
        "val_df = pd.read_csv(os.path.join(PROJECT_PATH, DATA_PATH, VAL_DATA_PATH))\n",
        "\n",
        "# modify test data frame for testing\n",
        "test_df[\"originalGrade\"] = test_df[\"meanGrade\"]\n",
        "test_df[\"meanGrade\"] = [0 for _ in range(len(test_df))]\n",
        "\n",
        "display(train_df[:5])\n",
        "# display(test_df)\n",
        "# display(val_df)\n"
      ],
      "metadata": {
        "id": "xcEFfuQZZGci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define dataset for data loader\n",
        "\n",
        "class HumicroeditDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # generate the complete new sentence as text and grade as label\n",
        "        new_sentence = re.sub(\"[</>]\", self.df[\"edit\"][idx], self.df[\"original\"][idx])\n",
        "        text = str(new_sentence)\n",
        "        label = self.df[\"meanGrade\"][idx]\n",
        "\n",
        "        # tokenization\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=MAX_LEN,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "            \"token_type_ids\": encoding[\"token_type_ids\"].flatten(),\n",
        "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "            \"label\": torch.tensor(label, dtype=torch.float)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "ac2i_ISdpgtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Setting and Training"
      ],
      "metadata": {
        "id": "A8UVvRSuojB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply BERT as tokenizer and model\n",
        "bert_tokenizer = transformers.BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "bert_model = transformers.BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=1)\n",
        "bert_model = bert_model.to(device)\n",
        "\n",
        "# freeze part of the BERT model but leave the input text representation unfreezed\n",
        "for name, param in bert_model.named_parameters():\n",
        "    if ((\"layer.10\" not in name) & (\"layer.11\" not in name) \n",
        "        & (\"pooler\" not in name) & (\"classifier\" not in name)):\n",
        "          param.requires_grad = False\n",
        "\n",
        "# generate data loader based on dataset\n",
        "train_ds = HumicroeditDataset(train_df, bert_tokenizer)\n",
        "test_ds = HumicroeditDataset(test_df, bert_tokenizer)\n",
        "val_ds = HumicroeditDataset(val_df, bert_tokenizer)\n",
        "train_dl = DataLoader(dataset=train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dl = DataLoader(dataset=test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_dl = DataLoader(dataset=val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# apply optimizer\n",
        "optimizer = transformers.AdamW(bert_model.parameters(), lr=LR)"
      ],
      "metadata": {
        "id": "fF9W6nWfjHaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = []\n",
        "val_loss = []\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "val_loss_min = 9999\n",
        "train_len = len(train_ds)\n",
        "val_len = len(val_ds)\n",
        "\n",
        "# get current time in format for naming\n",
        "time_zone = pytz.timezone(\"Asia/Singapore\")\n",
        "curr_time = datetime.now(time_zone).strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
        "file_name = str(curr_time) + (\".pt\")\n",
        "CURR_MODEL_PATH = os.path.join(PROJECT_PATH, MODEL_PATH, file_name)\n",
        "\n",
        "for epoch in range(EPOCH_NUM):\n",
        "\n",
        "    train_true = []\n",
        "    train_pred = []\n",
        "    val_true = []\n",
        "    val_pred = []\n",
        "\n",
        "    # training\n",
        "    start_time = time.time()\n",
        "    curr_train_loss = 0.0\n",
        "    bert_model.train()\n",
        "\n",
        "    # load data\n",
        "    for idx, train_data in enumerate(tqdm(train_dl)):\n",
        "        input_ids = train_data[\"input_ids\"].to(device)\n",
        "        token_type_ids = train_data[\"token_type_ids\"].to(device)\n",
        "        attention_mask = train_data[\"attention_mask\"].to(device)\n",
        "        label = train_data[\"label\"].to(device)\n",
        "        train_true += label.tolist()\n",
        "\n",
        "        # apply model\n",
        "        optimizer.zero_grad()\n",
        "        output = bert_model(\n",
        "            input_ids=input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=label\n",
        "        )\n",
        "        loss, logits = output[:2]\n",
        "        for logit in logits.reshape(-1):\n",
        "            train_pred.append(logit.item())\n",
        "        # loss = loss.float()\n",
        "        # loss = loss.to(torch.float32)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        curr_train_loss += loss.item()\n",
        "\n",
        "    train_loss.append(curr_train_loss/train_len)\n",
        "\n",
        "    # evaluating\n",
        "    curr_val_loss = 0.0\n",
        "    bert_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # load data\n",
        "        for idx, val_data in enumerate(tqdm(val_dl)):\n",
        "            input_ids = val_data[\"input_ids\"].to(device)\n",
        "            token_type_ids = val_data[\"token_type_ids\"].to(device)\n",
        "            attention_mask = val_data[\"attention_mask\"].to(device)\n",
        "            label = val_data[\"label\"].to(device)\n",
        "            val_true += label.tolist()\n",
        "\n",
        "            # apply model\n",
        "            output = bert_model(\n",
        "                input_ids=input_ids,\n",
        "                token_type_ids=token_type_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=label\n",
        "            )\n",
        "            loss, logits = output[:2]\n",
        "            curr_val_loss += loss.item()\n",
        "            for logit in logits.reshape(-1):\n",
        "                val_pred.append(logit.item())\n",
        "\n",
        "    val_loss.append(curr_val_loss/len(val_ds))\n",
        "\n",
        "    # save the model if optimized\n",
        "    if curr_val_loss/val_len < val_loss_min:\n",
        "        val_loss_min = curr_val_loss/val_len\n",
        "        torch.save(bert_model.state_dict(), CURR_MODEL_PATH)\n",
        "        print(f\"\\n++++++ Model optimized at epoch {epoch+1:02}! ++++++\\n\")\n",
        "\n",
        "    # calculate training and validation accuracy for this epoch\n",
        "    train_humor_true = [1 if float(i) > HUMOR_THRESHOLD else 0 for i in train_true]\n",
        "    train_humor_pred = [1 if float(i) > HUMOR_THRESHOLD else 0 for i in train_pred]\n",
        "    val_humor_true = [1 if float(i) > HUMOR_THRESHOLD else 0 for i in val_true]\n",
        "    val_humor_pred = [1 if float(i) > HUMOR_THRESHOLD else 0 for i in val_pred]\n",
        "    curr_train_acc = accuracy_score(train_humor_true, train_humor_pred)\n",
        "    curr_val_acc = accuracy_score(val_humor_true, val_humor_pred)\n",
        "    train_acc.append(curr_train_acc)\n",
        "    val_acc.append(curr_val_acc)\n",
        "\n",
        "    # record training and evaluation time\n",
        "    end_time = time.time()\n",
        "    time_interval = end_time - start_time\n",
        "    mins_interval = int(time_interval / 60)\n",
        "    secs_interval = int(time_interval - (mins_interval * 60))\n",
        "\n",
        "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {mins_interval}m {secs_interval}s\")\n",
        "    print(f\"\\tTrain Loss: {(curr_train_loss/train_len):.5f} | Train Acc: {curr_train_acc*100:.5f}%\")\n",
        "    print(f\"\\t Val. Loss: {(curr_val_loss/val_len):.5f} | Val. Acc: {curr_val_acc*100:.5f}%\\n\")\n"
      ],
      "metadata": {
        "id": "YCB52SCYY02y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot graph\n",
        "\n",
        "epochs = range(1, EPOCH_NUM+1, 1)\n",
        "max_loss = max(max(train_loss), max(val_loss))\n",
        "\n",
        "# plot loss\n",
        "fig, ax1 = plt.subplots()\n",
        "ax1.set_xlabel(\"Epoch\")\n",
        "ax1.set_ylabel(\"Loss\")\n",
        "ax1.set_ylim([0, max_loss+0.02])\n",
        "plt1 = ax1.plot(epochs, train_loss, 'yo-', label='Training Loss')\n",
        "plt2 = ax1.plot(epochs, val_loss, 'ro-', label='Validation Loss')\n",
        "\n",
        "# plot accuracy\n",
        "ax2 = ax1.twinx()\n",
        "ax2.set_ylabel(\"Accuracy\")\n",
        "ax2.set_ylim([0,1])\n",
        "plt3 = ax2.plot(epochs, train_acc, 'bo-', label='Training Accuracy')\n",
        "plt4 = ax2.plot(epochs, val_acc, 'co-', label='Validation Accuracy')\n",
        "\n",
        "plts = plt1 + plt2 + plt3 + plt4\n",
        "labs = [p.get_label() for p in plts]\n",
        "ax2.legend(plts, labs, loc=0)\n",
        "# fig.tight_layout()\n",
        "fig_name = \"Training and Validation Metrics\"\n",
        "plt.title(fig_name)\n",
        "plt.savefig(os.path.join(PROJECT_PATH, FIG_PATH, file_name.replace(\".pt\", \".png\")))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1wpXqfxiYPAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "MpcThIVioxM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "res = []\n",
        "bert_model.load_state_dict(torch.load(CURR_MODEL_PATH))\n",
        "bert_model.eval()   \n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    # load data\n",
        "    for idx, test_data in enumerate(tqdm(test_dl)):\n",
        "        input_ids = test_data[\"input_ids\"].to(device)\n",
        "        token_type_ids = test_data[\"token_type_ids\"].to(device)\n",
        "        attention_mask = test_data[\"attention_mask\"].to(device)\n",
        "        label = test_data[\"label\"].to(device)\n",
        "\n",
        "        # apply model\n",
        "        output = bert_model(\n",
        "            input_ids=input_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=label\n",
        "        )\n",
        "        loss, logits = output[:2]\n",
        "        for logit in logits.reshape(-1):\n",
        "            res.append(round(logit.item(), 5))\n",
        "\n",
        "# calculate RMSE loss\n",
        "rmse = np.sqrt(np.mean((np.array(\n",
        "    test_df[\"originalGrade\"].tolist()) - np.array(res)) ** 2))\n",
        "\n",
        "# add prediction and humor result in test dataframe\n",
        "test_df[\"predictedScores\"] = res\n",
        "test_df[\"ifHumorOriginal\"] = test_df[\"originalGrade\"].apply(\n",
        "    lambda x: 1 if float(x) > HUMOR_THRESHOLD else 0)\n",
        "test_df[\"ifHumorPredicted\"] = test_df[\"predictedScores\"].apply(\n",
        "    lambda x: 1 if float(x) > HUMOR_THRESHOLD else 0)\n",
        "test_df[\"meanGrade\"] = test_df[\"originalGrade\"].tolist()\n",
        "test_df = test_df.drop(\"originalGrade\", axis=1)\n",
        "\n",
        "display(test_df)\n",
        "test_df.to_csv(os.path.join(PROJECT_PATH, RES_CSV_PATH,\n",
        "                         file_name.replace(\".pt\", \".csv\")), index=False)\n",
        "\n",
        "# calculate metrics\n",
        "if_humor_true = test_df[\"ifHumorOriginal\"].tolist()\n",
        "if_humor_predicted = test_df[\"ifHumorPredicted\"].tolist()\n",
        "precision, recall, f1, support = precision_recall_fscore_support(\n",
        "    if_humor_true, if_humor_predicted, average='binary')\n",
        "\n",
        "print(f\"Test Results:\")\n",
        "print(f\"RMSE Loss: {rmse:.5f}\")\n",
        "print(f\"Precision: {precision:.5f}\")\n",
        "print(f\"Recall: {recall:.5f}\")\n",
        "print(f\"F1-measure: {f1:.5f}\")\n"
      ],
      "metadata": {
        "id": "gFrfaAOVoQZs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}